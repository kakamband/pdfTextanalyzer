{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": ".pdf_analizer_kernel",
   "display_name": ".pdf_analizer_kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss run contextual and spatial  approach #\n",
    "###  Main Alpha. Groping spatial search by topics, then extract spatial information related. Finally tokenize entities by a retrained nlp model ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load transfer learning model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loss runs extract target info by topics.\n",
    "\n",
    "@author: Eduardo Santos, Alberto de Obeso, Romulo Troncos\n",
    "date: Sep 20 \n",
    "\"\"\"\n",
    "import os \n",
    "import ast\n",
    "import lossrun\n",
    "from tensorflow.keras.utils import get_file\n",
    "import gensim\n",
    "import lossrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load re trained model\n",
    "try:\n",
    "    path = get_file('GoogleNews-vectors-negative300.bin.gz', \n",
    "        origin='https://s3.amazonaws.com/dl4j-distribution/'+'GoogleNews-vectors-negative300.bin.gz')\n",
    "except:\n",
    "    print('Error downloading')\n",
    "    raise\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.doesnt_match(string.split())\n",
    "\n",
    "#model.most_similar('date')\n",
    "#model.similarity('insurer','line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Line of  Policy Num retention Status Description Open By email  submitted  dated 3-Apr-17  Verna Bennett  wheelchair  onto the  was taken  Infirmary  fracture/dislocation Open Letter from  Patient  advising  malpractice  been filed  connection  occurred  fell from  being offloaded    Certain Underwriters at Lloyds - Certain\n..................................................\nContextual realed words to insurer: ['Policy', 'retention', 'Status', 'the', 'was', 'been', 'filed', 'connection', 'being']\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Study relation to topics \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "string = 'Line of  Policy Num retention Status Description Open By email  submitted  dated 3-Apr-17  Verna Bennett  wheelchair  onto the  was taken  Infirmary  fracture/dislocation Open Letter from  Patient  advising  malpractice  been filed  connection  occurred  fell from  being offloaded    Certain Underwriters at Lloyds - Certain'\n",
    "\n",
    "\n",
    "\n",
    "# create a list for correlation score\n",
    "contextual_relation_score = []\n",
    "contextual_relation_word = []\n",
    "\n",
    "# take every \\w in text\n",
    "elements_in_split = string.split()\n",
    "\n",
    "relation_ratio = .1 # grammar correlation from -1 to 1\n",
    "# filter \n",
    "for i in elements_in_split:\n",
    "\n",
    "    try: \n",
    "        correlation = model.similarity('status', i)\n",
    "                                        #topic  ,word \n",
    "        if (correlation > relation_ratio):\n",
    "            contextual_relation_word.append(i)\n",
    "            contextual_relation_score.append(correlation)\n",
    "    except:\n",
    "        #print(i + ' most be retrained or modify')\n",
    "        contextual_relation_score.append(-1)\n",
    "        pass\n",
    "\n",
    "print(string)\n",
    "print('.'*50)\n",
    "print('Contextual realed words to insurer: ' + f'{contextual_relation_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('insurers', 0.7716545462608337),\n ('Insurer', 0.7563024759292603),\n ('reinsurer', 0.7067841291427612),\n ('insurance', 0.7065000534057617),\n ('casualty_insurer', 0.6807657480239868),\n ('policyholders', 0.6805585622787476),\n ('Insurers', 0.6498398184776306),\n ('Insurance', 0.6475108861923218),\n ('policyholder', 0.6436665058135986),\n ('premiums', 0.615146279335022)]"
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "## \n",
    "model.most_similar('insurer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NLP entitites model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools # join list trhougth permutations\n",
    "import numpy as np  # just in case math is needed\n",
    "import random # shuffle index tool\n",
    "import csv # cvs file format managemnent\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate data files\n",
    " \n",
    "# create the days list\n",
    "days = [str(i) for i in range(1,32)]\n",
    " \n",
    "# create the monts list\n",
    "monts = [str(i) for i in range(1,13)]\n",
    " \n",
    "# crete the years list\n",
    "years = [str(1900 + i) for i in range(0,150)]\n",
    " \n",
    "# create the monts list in letters\n",
    "monts_letter = ['Jan','Feb','Mar','Apr', 'MAY','JUN', 'JUL', 'AUG','sep','oct','nov','dec']\n",
    " \n",
    "# join the monts width the days\n",
    "days_monts = [day + '/' + mont for day in days for mont in monts] + [mont + '/' + day for day in days for mont in monts]\n",
    " \n",
    "# joind days_monts to years\n",
    "data_date_diag = [day_mont + '/' + year for day_mont in days_monts for year in years]# + [year + '/' + day for day in days for mont in monts]\n",
    " \n",
    "# change the format diag for dash\n",
    "data_date_dash = []\n",
    " \n",
    "for i in range(len(data_date_diag)):\n",
    "  data_date_dash.append(data_date_diag[i].replace('/','-'))\n",
    " \n",
    "# create the days_monts whith letter\n",
    " \n",
    "days_monts_letter = [day + '/' + mont for day in days for mont in monts_letter] + [mont + '/' + day for day in days for mont in monts_letter]\n",
    " \n",
    "data_date_letter_diag = [day_mont + '/' + year for day_mont in days_monts_letter for year in years]\n",
    " \n",
    "data_date_letter_dash = []\n",
    " \n",
    "for i in range(len(data_date_diag)):\n",
    "  data_date_letter_dash.append(data_date_letter_diag[i].replace('/','-'))\n",
    " \n",
    "# join all formats\n",
    " \n",
    "data_date = data_date_diag + data_date_dash + data_date_letter_diag + data_date_letter_dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load external data from web scraping\n",
    " \n",
    "DATA_PATH = 'data/data_base/'\n",
    " \n",
    "_lists2list = lambda List: [item for sublist in List for item in sublist]\n",
    " \n",
    " \n",
    "# \n",
    "#     DATA DISTRIBUTUION AS.\n",
    "#      _______________    ___________________     ___________________________\n",
    "#     |READ CVS FILES| => |TRANSFORM TO LIST|  => |PRE PROC LIST, CLEAN DATA|\n",
    "#   \n",
    "#      __________________________    __________\n",
    "#     |MERGE NAMES AND LAS NAMES| => |SAVE DATA|\n",
    " \n",
    "# read companies list \n",
    "with open(DATA_PATH + 'loss_runs _companies.csv', newline= '' ) as f:\n",
    "  reader = csv.reader(f)\n",
    "  data_companies = list(reader)\n",
    " \n",
    "# read last names list\n",
    "with open(DATA_PATH + 'loss_runs_last_name.csv', newline= '' ) as f:\n",
    "  reader = csv.reader(f)\n",
    "  data_lastnames = list(reader)\n",
    " \n",
    "# read last names\n",
    "with open(DATA_PATH + 'loss_runs_names.csv', newline= '' ) as f:\n",
    "  reader = csv.reader(f)\n",
    "  data_names = list(reader)\n",
    " \n",
    "# list of list to list\n",
    " \n",
    "# compnanies\n",
    "data_companies_list = _lists2list(data_companies)\n",
    "# last names\n",
    "data_lastnames_list = _lists2list(data_lastnames)\n",
    "# names\n",
    "data_names_list = _lists2list(data_names)\n",
    " \n",
    " \n",
    "# join names and last names lists\n",
    " \n",
    "data_names = [name + ' ' + last for name in data_names_list for last in data_lastnames_list]\n",
    "random.shuffle(data_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean companies names\n",
    "import string \n",
    "\n",
    "printable= set(string.printable)\n",
    "\n",
    "#sent = data_companies_list[696]\n",
    "\n",
    "for i in range(len(data_companies_list)):\n",
    "   data_companies_list[i] = ''.join(filter(lambda x: x in printable, data_companies_list[i]))\n",
    "\n",
    "data_companies = data_companies_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrain model\n",
    "# random insert abbreviation last names (i.e., Alexander P.)\n",
    "\n",
    "random.shuffle(data_date)\n",
    "\n",
    "TRAIN_DATA = []\n",
    "\n",
    "for i in range(1000):\n",
    "  #print(data_date[i])\n",
    "  TRAIN_DATA.append((data_date[i], {\"entities\":[(0,len(data_date[i]),'DATE')]}))\n",
    "\n",
    "for i in range(len(data_companies)):\n",
    "  #print(data_date[i])\n",
    "  TRAIN_DATA.append((data_companies[i], {\"entities\":[(0,len(data_companies[i]),'INSURER')]}))\n",
    "\n",
    "for i in range(500):\n",
    "  #print(data_date[i])\n",
    "  TRAIN_DATA.append((data_names[i], {\"entities\":[(0,len(data_names[i]),'NAME')]}))\n",
    "\n",
    "\n",
    "random.shuffle(TRAIN_DATA)\n",
    "TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load existing spacy model for retraining\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "# Getting the pipeline component\n",
    "ner=nlp.get_pipe(\"ner\")\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "  for ent in annotations.get(\"entities\"):\n",
    "    ner.add_label(ent[2])\n",
    "\n",
    "# disable pipe lines\n",
    "# Disable pipeline components you dont need to change\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " {'ner': 3286.538015978205}\nLosses {'ner': 3296.8200109170552}\nLosses {'ner': 3308.7738002108213}\nLosses {'ner': 3313.4043469892617}\nLosses {'ner': 3325.872245238888}\nLosses {'ner': 3336.033420489895}\nLosses {'ner': 3341.511309372055}\nLosses {'ner': 3354.3337333069917}\nLosses {'ner': 3362.5549117492315}\nLosses {'ner': 3371.705316560137}\nLosses {'ner': 3385.0202404426213}\nLosses {'ner': 3396.6756802962896}\nLosses {'ner': 3408.6644342111226}\nLosses {'ner': 3417.034900324213}\nLosses {'ner': 3421.5455857680913}\nLosses {'ner': 3431.987428979742}\nLosses {'ner': 3437.470235125887}\nLosses {'ner': 3447.1773476766225}\nLosses {'ner': 3451.3503992127057}\nLosses {'ner': 3469.349338726866}\nLosses {'ner': 3483.0752720402356}\nLosses {'ner': 3486.489025401153}\nLosses {'ner': 3494.794946836509}\nLosses {'ner': 3503.3968552818674}\nLosses {'ner': 3515.6513594856638}\nLosses {'ner': 3522.501415714823}\nLosses {'ner': 3527.997783825003}\nLosses {'ner': 3542.4281642289984}\nLosses {'ner': 3561.3186462374556}\nLosses {'ner': 3570.400375709139}\nLosses {'ner': 3583.0113063903677}\nLosses {'ner': 3595.6023226829398}\nLosses {'ner': 3607.3288174124586}\nLosses {'ner': 3617.277790114485}\nLosses {'ner': 3632.880733058058}\nLosses {'ner': 3644.4449839444983}\nLosses {'ner': 3656.9652176948416}\nLosses {'ner': 3663.500123366915}\nLosses {'ner': 3675.8619718106615}\nLosses {'ner': 3684.0808844121325}\nLosses {'ner': 3695.889807954632}\nLosses {'ner': 3711.327121511303}\nLosses {'ner': 3728.534595743023}\nLosses {'ner': 3737.852024030142}\nLosses {'ner': 3743.4272406401265}\nLosses {'ner': 3755.02921408435}\nLosses {'ner': 3767.8880375625718}\nLosses {'ner': 3779.0648291113007}\nLosses {'ner': 3788.687642395698}\nLosses {'ner': 3800.405491650306}\nLosses {'ner': 3810.575714171134}\nLosses {'ner': 3820.789813787878}\nLosses {'ner': 3831.555710763872}\nLosses {'ner': 3844.4999375056627}\nLosses {'ner': 3858.103598208368}\nLosses {'ner': 3874.897032470644}\nLosses {'ner': 3890.052744836748}\nLosses {'ner': 3898.0516739200953}\nLosses {'ner': 3909.7948416780832}\nLosses {'ner': 3920.5993156537565}\nLosses {'ner': 3936.4879882440123}\nLosses {'ner': 3946.939985047296}\nLosses {'ner': 3961.389939557031}\nLosses {'ner': 3974.2902896508726}\nLosses {'ner': 3990.592832814172}\nLosses {'ner': 4000.209828029588}\nLosses {'ner': 4008.1918930068764}\nLosses {'ner': 4022.5951684966835}\nLosses {'ner': 4041.019558812693}\nLosses {'ner': 4055.595087195948}\nLosses {'ner': 4063.487298215941}\nLosses {'ner': 4076.01135834606}\nLosses {'ner': 4084.929858143643}\nLosses {'ner': 4094.6771605566296}\nLosses {'ner': 4109.277149017171}\nLosses {'ner': 4125.134950931386}\nLosses {'ner': 4132.528734977559}\nLosses {'ner': 4143.301318641023}\nLosses {'ner': 4153.17752167137}\nLosses {'ner': 4168.2209540262975}\nLosses {'ner': 4176.182352694885}\nLosses {'ner': 4196.966725978271}\nLosses {'ner': 4205.974457713977}\nLosses {'ner': 4219.641443434373}\nLosses {'ner': 4228.266050401346}\nLosses {'ner': 4247.19151050152}\nLosses {'ner': 4256.564484271423}\nLosses {'ner': 4267.255819472686}\nLosses {'ner': 4278.054983751119}\nLosses {'ner': 4289.233896748365}\nLosses {'ner': 4298.158410803617}\nLosses {'ner': 4302.420542017759}\nLosses {'ner': 4315.975196854413}\nLosses {'ner': 4326.72643746549}\nLosses {'ner': 4335.499354378522}\nLosses {'ner': 4346.398667470754}\nLosses {'ner': 4360.330828563512}\nLosses {'ner': 4371.515400246919}\nLosses {'ner': 4384.443418220819}\nLosses {'ner': 4391.5258540671475}\nLosses {'ner': 8.278125613927841}\nLosses {'ner': 12.65491711091812}\nLosses {'ner': 22.604096194027022}\nLosses {'ner': 32.15947790932472}\nLosses {'ner': 38.22569241118248}\nLosses {'ner': 42.431634207485274}\nLosses {'ner': 52.32931711268242}\nLosses {'ner': 61.87604238581474}\nLosses {'ner': 69.29198624682243}\nLosses {'ner': 74.15559511375244}\nLosses {'ner': 82.5898322511893}\nLosses {'ner': 90.60563874733265}\nLosses {'ner': 95.68937290203388}\nLosses {'ner': 105.11680984985645}\nLosses {'ner': 112.56987566803213}\nLosses {'ner': 121.48822105233188}\nLosses {'ner': 125.12831821129288}\nLosses {'ner': 134.34267993853058}\nLosses {'ner': 144.49388720676865}\nLosses {'ner': 150.63761975452866}\nLosses {'ner': 156.50609900639023}\nLosses {'ner': 166.52185382530655}\nLosses {'ner': 178.58176901028122}\nLosses {'ner': 180.81609823391415}\nLosses {'ner': 189.65992881939388}\nLosses {'ner': 193.16038947389103}\nLosses {'ner': 200.73475842759586}\nLosses {'ner': 205.71597545115085}\nLosses {'ner': 223.12820212809177}\nLosses {'ner': 232.8776770111713}\nLosses {'ner': 244.15769677130314}\nLosses {'ner': 252.97601525751682}\nLosses {'ner': 258.6160215817931}\nLosses {'ner': 263.93685818543463}\nLosses {'ner': 271.0702607714179}\nLosses {'ner': 281.1461781345847}\nLosses {'ner': 291.3869342886451}\nLosses {'ner': 296.32976654983884}\nLosses {'ner': 306.1322872421873}\nLosses {'ner': 314.0870865742903}\nLosses {'ner': 320.27871905744485}\nLosses {'ner': 330.97763656557015}\nLosses {'ner': 339.53065820515565}\nLosses {'ner': 351.92396064579896}\nLosses {'ner': 359.5267141759772}\nLosses {'ner': 365.38991495970674}\nLosses {'ner': 375.7306105792899}\nLosses {'ner': 380.99463960532137}\nLosses {'ner': 388.02711227539965}\nLosses {'ner': 397.2049364149947}\nLosses {'ner': 403.02344875041433}\nLosses {'ner': 417.90465335551687}\nLosses {'ner': 423.22576789561697}\nLosses {'ner': 427.2445044368882}\nLosses {'ner': 438.0044418424744}\nLosses {'ner': 444.63680975142904}\nLosses {'ner': 454.45768670741506}\nLosses {'ner': 458.43913279477545}\nLosses {'ner': 463.335195638277}\nLosses {'ner': 479.2758834138889}\nLosses {'ner': 484.5312769189853}\nLosses {'ner': 491.3130144849796}\nLosses {'ner': 495.1441592984278}\nLosses {'ner': 499.31179760027715}\nLosses {'ner': 509.0481389575083}\nLosses {'ner': 517.4378084317226}\nLosses {'ner': 525.8871574029703}\nLosses {'ner': 534.8205157146234}\nLosses {'ner': 546.9417590722818}\nLosses {'ner': 552.8731561615508}\nLosses {'ner': 560.7143498375457}\nLosses {'ner': 566.2421085699838}\nLosses {'ner': 574.6824853285592}\nLosses {'ner': 584.4705124362748}\nLosses {'ner': 592.6490561231416}\nLosses {'ner': 602.6620105370324}\nLosses {'ner': 606.3471770151932}\nLosses {'ner': 615.5779306753952}\nLosses {'ner': 621.2618977292855}\nLosses {'ner': 625.3021500572045}\nLosses {'ner': 633.2741460516531}\nLosses {'ner': 634.7363725275914}\nLosses {'ner': 641.8235350222508}\nLosses {'ner': 646.1524075241009}\nLosses {'ner': 651.8817962379376}\nLosses {'ner': 662.1579879493634}\nLosses {'ner': 672.8490258426586}\nLosses {'ner': 681.2676201434056}\nLosses {'ner': 693.3400230975071}\nLosses {'ner': 703.0787786812703}\nLosses {'ner': 717.2908206553379}\nLosses {'ner': 725.3385743842641}\nLosses {'ner': 736.8182720885793}\nLosses {'ner': 744.0442855701963}\nLosses {'ner': 753.3339534112433}\nLosses {'ner': 757.3387062407105}\nLosses {'ner': 761.805437431392}\nLosses {'ner': 775.020760402736}\nLosses {'ner': 780.6382025337784}\nLosses {'ner': 790.2784060097306}\nLosses {'ner': 796.4447077370255}\nLosses {'ner': 806.2473579502671}\nLosses {'ner': 810.720851168689}\nLosses {'ner': 824.9950215674012}\nLosses {'ner': 836.9238176203339}\nLosses {'ner': 848.8017567253678}\nLosses {'ner': 853.6953978992074}\nLosses {'ner': 864.5014365172951}\nLosses {'ner': 868.8461661703891}\nLosses {'ner': 879.1864440806216}\nLosses {'ner': 888.0190911658115}\nLosses {'ner': 896.0792304642505}\nLosses {'ner': 904.489851749689}\nLosses {'ner': 911.122899509222}\nLosses {'ner': 917.1078866226867}\nLosses {'ner': 928.5498471958831}\nLosses {'ner': 933.8182224137977}\nLosses {'ner': 945.9417690141395}\nLosses {'ner': 951.3276455430582}\nLosses {'ner': 959.3410731433228}\nLosses {'ner': 967.4683870561689}\nLosses {'ner': 974.6332495027723}\nLosses {'ner': 979.0785415583791}\nLosses {'ner': 986.9419406551438}\nLosses {'ner': 990.5721440527391}\nLosses {'ner': 995.0478589746904}\nLosses {'ner': 1004.9134578916978}\nLosses {'ner': 1011.0377789947938}\nLosses {'ner': 1015.5727859351587}\nLosses {'ner': 1020.8797870676767}\nLosses {'ner': 1033.8630603354227}\nLosses {'ner': 1040.2362237971079}\nLosses {'ner': 1045.7600308161032}\nLosses {'ner': 1053.5503146152746}\nLosses {'ner': 1061.692364643122}\nLosses {'ner': 1071.8194045524847}\nLosses {'ner': 1078.6207699280035}\nLosses {'ner': 1082.5524462794688}\nLosses {'ner': 1090.0126049851801}\nLosses {'ner': 1094.2672449786019}\nLosses {'ner': 1103.7610179025482}\nLosses {'ner': 1108.2340183689062}\nLosses {'ner': 1119.0564614965383}\nLosses {'ner': 1131.9595661594335}\nLosses {'ner': 1139.4734681977693}\nLosses {'ner': 1148.5057550801698}\nLosses {'ner': 1155.0628088845674}\nLosses {'ner': 1163.610967232174}\nLosses {'ner': 1170.5055290593568}\nLosses {'ner': 1179.331555439419}\nLosses {'ner': 1187.9983317239944}\nLosses {'ner': 1198.6206023081008}\nLosses {'ner': 1206.2764046250645}\nLosses {'ner': 1211.750025457031}\nLosses {'ner': 1216.76333839286}\nLosses {'ner': 1227.095407253391}\nLosses {'ner': 1231.3882346696023}\nLosses {'ner': 1239.325610870755}\nLosses {'ner': 1247.749488527215}\nLosses {'ner': 1255.1934619343642}\nLosses {'ner': 1261.8817734106246}\nLosses {'ner': 1269.833512721843}\nLosses {'ner': 1277.7446524484817}\nLosses {'ner': 1286.3310015185539}\nLosses {'ner': 1295.000011919326}\nLosses {'ner': 1301.5195767863456}\nLosses {'ner': 1309.30631753642}\nLosses {'ner': 1315.302835135049}\nLosses {'ner': 1327.244420079774}\nLosses {'ner': 1337.3388079090778}\nLosses {'ner': 1346.1583851261798}\nLosses {'ner': 1352.664889304227}\nLosses {'ner': 1361.895075170583}\nLosses {'ner': 1364.381363837308}\nLosses {'ner': 1377.295067755765}\nLosses {'ner': 1383.2659904046718}\nLosses {'ner': 1390.7453456459675}\nLosses {'ner': 1396.1820619333644}\nLosses {'ner': 1398.185304274478}\nLosses {'ner': 1400.9321307155053}\nLosses {'ner': 1413.8299988719384}\nLosses {'ner': 1419.758259198324}\nLosses {'ner': 1430.0993864747445}\nLosses {'ner': 1436.4477061244409}\nLosses {'ner': 1441.2181949230592}\nLosses {'ner': 1450.42365046467}\nLosses {'ner': 1457.530304155008}\nLosses {'ner': 1463.6696292969148}\nLosses {'ner': 1471.3655550095002}\nLosses {'ner': 1476.3531545397575}\nLosses {'ner': 1479.7717930075462}\nLosses {'ner': 1493.7602707144554}\nLosses {'ner': 1501.927295947143}\nLosses {'ner': 1508.1252496424995}\nLosses {'ner': 1515.6099165860496}\nLosses {'ner': 1521.1924138847671}\nLosses {'ner': 1531.5591537538849}\nLosses {'ner': 1533.7457641396707}\nLosses {'ner': 1539.0942661199754}\nLosses {'ner': 1545.0847461257165}\nLosses {'ner': 1549.7692184481805}\nLosses {'ner': 1555.2194180820172}\nLosses {'ner': 1561.4155082438176}\nLosses {'ner': 1571.885533902425}\nLosses {'ner': 1581.6259247992223}\nLosses {'ner': 1586.4212491497258}\nLosses {'ner': 1592.6240739807347}\nLosses {'ner': 1600.6960076078633}\nLosses {'ner': 1608.5003818258504}\nLosses {'ner': 1619.1461322292546}\nLosses {'ner': 1625.747599183009}\nLosses {'ner': 1627.9955773934582}\nLosses {'ner': 1638.8772408351163}\nLosses {'ner': 1640.9148762211064}\nLosses {'ner': 1644.2684980336437}\nLosses {'ner': 1655.9780944067845}\nLosses {'ner': 1666.222017832459}\nLosses {'ner': 1670.4076787311444}\nLosses {'ner': 1679.3449297029385}\nLosses {'ner': 1686.6254799742112}\nLosses {'ner': 1692.334591903151}\nLosses {'ner': 1700.60533145851}\nLosses {'ner': 1707.0718590397248}\nLosses {'ner': 1714.0009634870896}\nLosses {'ner': 1722.745236434401}\nLosses {'ner': 1732.0625114220986}\nLosses {'ner': 1739.0711384791741}\nLosses {'ner': 1750.431550123156}\nLosses {'ner': 1756.727996744574}\nLosses {'ner': 1762.9175171334157}\nLosses {'ner': 1773.7516917887936}\nLosses {'ner': 1787.62990734017}\nLosses {'ner': 1800.281082439595}\nLosses {'ner': 1808.9658630372817}\nLosses {'ner': 1820.8015288593108}\nLosses {'ner': 1829.5873660029704}\nLosses {'ner': 1842.88050259965}\nLosses {'ner': 1852.1102920951182}\nLosses {'ner': 1866.316099561702}\nLosses {'ner': 1876.0933674945938}\nLosses {'ner': 1882.5705972089875}\nLosses {'ner': 1892.1156618967163}\nLosses {'ner': 1901.2696245386708}\nLosses {'ner': 1916.825182028066}\nLosses {'ner': 1931.5131048038113}\nLosses {'ner': 1939.0346027984726}\nLosses {'ner': 1951.983040727626}\nLosses {'ner': 1963.4299295916665}\nLosses {'ner': 1974.546120144378}\nLosses {'ner': 1978.420991986941}\nLosses {'ner': 1985.0125265418994}\nLosses {'ner': 1993.0840376913059}\nLosses {'ner': 2008.9074708520877}\nLosses {'ner': 2020.2445934712398}\nLosses {'ner': 2030.867120832156}\nLosses {'ner': 2039.8268600403774}\nLosses {'ner': 2047.779249817084}\nLosses {'ner': 2063.78468027706}\nLosses {'ner': 2073.2797004281983}\nLosses {'ner': 2085.6412904560075}\nLosses {'ner': 2096.3902380167947}\nLosses {'ner': 2105.2354643418894}\nLosses {'ner': 2116.5843668952093}\nLosses {'ner': 2126.621199466239}\nLosses {'ner': 2138.230663396369}\nLosses {'ner': 2148.7354018464193}\nLosses {'ner': 2160.358530797015}\nLosses {'ner': 2165.552822340929}\nLosses {'ner': 2170.972049106562}\nLosses {'ner': 2175.003474403822}\nLosses {'ner': 2184.7468768625495}\nLosses {'ner': 2193.0481269626853}\nLosses {'ner': 2205.7204907207724}\nLosses {'ner': 2221.6661577491996}\nLosses {'ner': 2232.6507634787795}\nLosses {'ner': 2244.344157186055}\nLosses {'ner': 2252.432871040606}\nLosses {'ner': 2263.9536860703465}\nLosses {'ner': 2269.4421584128377}\nLosses {'ner': 2280.8101587413785}\nLosses {'ner': 2285.347250912988}\nLosses {'ner': 2293.5414477928516}\nLosses {'ner': 2300.389249656999}\nLosses {'ner': 2310.7431525631782}\nLosses {'ner': 2320.833144013488}\nLosses {'ner': 2329.8419566794273}\nLosses {'ner': 2333.768264478568}\nLosses {'ner': 2340.1900704563895}\nLosses {'ner': 2348.9299741448203}\nLosses {'ner': 2360.17390007389}\nLosses {'ner': 2368.2060162485877}\nLosses {'ner': 2377.8401436270515}\nLosses {'ner': 2382.121123994265}\nLosses {'ner': 2391.5850045332054}\nLosses {'ner': 2405.3814605125526}\nLosses {'ner': 2415.5697256573776}\nLosses {'ner': 2426.8904769667724}\nLosses {'ner': 2441.7342516430954}\nLosses {'ner': 2455.1802715310196}\nLosses {'ner': 2464.7168793925384}\nLosses {'ner': 2469.6660385378937}\nLosses {'ner': 2476.320505077515}\nLosses {'ner': 2485.53262388197}\nLosses {'ner': 2496.567292685185}\nLosses {'ner': 2511.396245444689}\nLosses {'ner': 2518.935748843972}\nLosses {'ner': 2526.027012972657}\nLosses {'ner': 2537.3836236284415}\nLosses {'ner': 2541.943735078182}\nLosses {'ner': 2553.5962277205845}\nLosses {'ner': 2564.9522290380855}\nLosses {'ner': 2577.3349180372616}\nLosses {'ner': 2588.512519076671}\nLosses {'ner': 2600.485668018665}\nLosses {'ner': 2607.9868984383947}\nLosses {'ner': 2622.7411544484503}\nLosses {'ner': 2635.886573569239}\nLosses {'ner': 2640.404837147654}\nLosses {'ner': 2650.996643559397}\nLosses {'ner': 2660.392402188242}\nLosses {'ner': 2670.9342069489367}\nLosses {'ner': 2681.4179967505343}\nLosses {'ner': 2689.790584610165}\nLosses {'ner': 2693.31655328101}\nLosses {'ner': 2714.4134568435557}\nLosses {'ner': 2721.3448796731836}\nLosses {'ner': 2733.907570527256}\nLosses {'ner': 2744.2470969421274}\nLosses {'ner': 2752.7580604267723}\nLosses {'ner': 2758.4934439671642}\nLosses {'ner': 2765.38202968365}\nLosses {'ner': 2774.567907945574}\nLosses {'ner': 2783.617117942632}\nLosses {'ner': 2796.312292994321}\nLosses {'ner': 2800.790521375869}\nLosses {'ner': 2807.5130914375527}\nLosses {'ner': 2812.0471889302476}\nLosses {'ner': 2819.387131802772}\nLosses {'ner': 2829.6885152981026}\nLosses {'ner': 2843.445269100402}\nLosses {'ner': 2852.3464388475163}\nLosses {'ner': 2863.5728009567006}\nLosses {'ner': 2871.9558708295567}\nLosses {'ner': 2882.021134160493}\nLosses {'ner': 2891.500790618394}\nLosses {'ner': 2899.0666436180813}\nLosses {'ner': 2911.831207059358}\nLosses {'ner': 2919.669243834947}\nLosses {'ner': 2921.5430546030743}\nLosses {'ner': 2929.9034068212254}\nLosses {'ner': 2937.309653066133}\nLosses {'ner': 2943.873433493112}\nLosses {'ner': 2960.3941405639393}\nLosses {'ner': 2967.960948549245}\nLosses {'ner': 2980.6581723317845}\nLosses {'ner': 2994.4106746539815}\nLosses {'ner': 3006.706701718305}\nLosses {'ner': 3016.07583983991}\nLosses {'ner': 3022.934454880689}\nLosses {'ner': 3030.9335699808194}\nLosses {'ner': 3038.5131470095707}\nLosses {'ner': 3044.430638147552}\nLosses {'ner': 3061.9089506399228}\nLosses {'ner': 3074.4669774305416}\nLosses {'ner': 3081.601802898605}\nLosses {'ner': 3089.7984405290676}\nLosses {'ner': 3092.845092667301}\nLosses {'ner': 3102.1597020756794}\nLosses {'ner': 3113.5777802002026}\nLosses {'ner': 3114.212572721291}\nLosses {'ner': 3125.0073745525365}\nLosses {'ner': 3132.7548777735715}\nLosses {'ner': 3137.3962815715854}\nLosses {'ner': 3143.8872563405816}\nLosses {'ner': 3156.528185979921}\nLosses {'ner': 3164.778033630449}\nLosses {'ner': 3181.430642025072}\nLosses {'ner': 3197.360963599283}\nLosses {'ner': 3209.7745969458406}\nLosses {'ner': 3219.4413532181566}\nLosses {'ner': 3232.0346230431383}\nLosses {'ner': 3239.7067203684633}\nLosses {'ner': 3244.4889277382676}\nLosses {'ner': 3253.5500406301503}\nLosses {'ner': 3262.8460638797765}\nLosses {'ner': 3276.6282471693044}\nLosses {'ner': 3289.618613804508}\nLosses {'ner': 3303.113220759485}\nLosses {'ner': 3310.815724738691}\nLosses {'ner': 3320.9966664396266}\nLosses {'ner': 3331.2233259163836}\nLosses {'ner': 3338.677498050783}\nLosses {'ner': 3347.590018697832}\nLosses {'ner': 3358.5632655106524}\nLosses {'ner': 3360.545024641607}\nLosses {'ner': 3369.131289758537}\nLosses {'ner': 3380.332706489418}\nLosses {'ner': 3389.2810613296965}\nLosses {'ner': 3400.7653462074736}\nLosses {'ner': 3411.39199284539}\nLosses {'ner': 3419.1586749635676}\nLosses {'ner': 3429.5680959306696}\nLosses {'ner': 3437.874816664312}\nLosses {'ner': 3451.630984135721}\nLosses {'ner': 3458.0556946478823}\nLosses {'ner': 3468.0563599549273}\nLosses {'ner': 3480.5151829205493}\nLosses {'ner': 3491.5674105368594}\nLosses {'ner': 3501.9470875941256}\nLosses {'ner': 3506.3551022822635}\nLosses {'ner': 3526.354818325736}\nLosses {'ner': 3538.5663186127917}\nLosses {'ner': 3546.484120656051}\nLosses {'ner': 3560.003371168174}\nLosses {'ner': 3570.7562871168034}\nLosses {'ner': 3582.253926296033}\nLosses {'ner': 3595.5309337566273}\nLosses {'ner': 3611.294812102117}\nLosses {'ner': 3622.8890042493717}\nLosses {'ner': 3636.0497635791676}\nLosses {'ner': 3647.7465866992848}\nLosses {'ner': 3666.481996793546}\nLosses {'ner': 3681.7253797243015}\nLosses {'ner': 3693.428670708336}\nLosses {'ner': 3701.2189126762228}\nLosses {'ner': 3706.772146061309}\nLosses {'ner': 3717.446733609088}\nLosses {'ner': 3724.0394383224325}\nLosses {'ner': 3736.965614989646}\nLosses {'ner': 3744.8375255020933}\nLosses {'ner': 3762.371188774951}\nLosses {'ner': 3770.915847733863}\nLosses {'ner': 3785.692721441634}\nLosses {'ner': 3796.994405586053}\nLosses {'ner': 3804.3205419366845}\nLosses {'ner': 3818.3525429552087}\nLosses {'ner': 3834.9523076003084}\nLosses {'ner': 3842.5250888054857}\nLosses {'ner': 3851.376460272599}\nLosses {'ner': 3859.350277892327}\nLosses {'ner': 3871.5375800049064}\nLosses {'ner': 3884.6701447880027}\nLosses {'ner': 3898.2065809881447}\nLosses {'ner': 3911.7715676939247}\nLosses {'ner': 3925.6528140222786}\nLosses {'ner': 3940.369953266358}\nLosses {'ner': 3949.7484017973898}\nLosses {'ner': 3960.252038977122}\nLosses {'ner': 3973.421286365962}\nLosses {'ner': 3987.9003789639232}\nLosses {'ner': 3998.8010273194072}\nLosses {'ner': 4013.142127535319}\nLosses {'ner': 4028.1386816715954}\nLosses {'ner': 4037.936464688754}\nLosses {'ner': 4048.650522670722}\nLosses {'ner': 4061.858342013335}\nLosses {'ner': 4079.6240688180683}\nLosses {'ner': 4088.1620100354908}\nLosses {'ner': 4101.171101412749}\nLosses {'ner': 4120.738981327986}\nLosses {'ner': 4127.830821818685}\nLosses {'ner': 4138.634992933845}\nLosses {'ner': 4155.452924347495}\nLosses {'ner': 4170.294302321052}\nLosses {'ner': 4179.097180820083}\nLosses {'ner': 4197.2865571743005}\nLosses {'ner': 4213.09608063755}\nLosses {'ner': 4228.565648651695}\nLosses {'ner': 4242.1206140285485}\nLosses {'ner': 4254.731469488715}\nLosses {'ner': 4262.667526102638}\nLosses {'ner': 4282.795465088462}\nLosses {'ner': 4298.010242915725}\nLosses {'ner': 4309.268375671481}\nLosses {'ner': 4316.711128968758}\nLosses {'ner': 4327.738561008138}\nLosses {'ner': 4334.186181757612}\nLosses {'ner': 4345.271836433572}\nLosses {'ner': 4355.1582496045285}\nLosses {'ner': 4364.107311892336}\nLosses {'ner': 4367.252529865376}\n"
    }
   ],
   "source": [
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "  # Training for 30 iterations\n",
    "  for iteration in range(30):\n",
    "\n",
    "    # shuufling examples  before every iteration\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "text = 'Pedro Ramirex'\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)\n",
    "displacy.serve(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"Carrier:  Line of  Policy Num retention Status Description Open By email  submitted  dated 3-Apr-17  Verna Bennett  wheelchair  onto the  was taken  Infirmary  fracture/dislocation Open Letter from  Patient's  advising  malpractice  been filed  connect  occurred  fell from  being offloaded    Certain Underwriters at Lloyds - Certain    Loss Report prepared for A-MMED Ambulance Inc., DBA A-Med Carrier:\""
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "string"
   ]
  }
 ]
}